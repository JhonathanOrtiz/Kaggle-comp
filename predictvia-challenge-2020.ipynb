{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://www.blumeglobal.com/wp-content/uploads/2018/11/NLP-image.jpg)"},{"metadata":{},"cell_type":"markdown","source":"#  <h1>INTRODUCCIÓN A LOS DATOS SECUENCIALES Y AL ANÁLISIS DE TEXTO</h1>\n\nEl texto es un tipo de dato bastante particular, ya que fue diseñado para nosotros como humanos podamos conumicarnos. Sin embargo, las computadoras no entienden éste lenguaje (Primer problema). El analisis se vuelve tan complejo como el idioma en el que se esté trabaajando (Segundo problema). La buena noticia es que con la ayuda de técnica de análisis de datos y las mejoras en los algoritmos de procesamiento de lenguaje natural éstos problemas han quedado en el pasado. Dichas técnicas las exploraremos en el siguiente notebook hasta llegar a como detectar si un tweet es generado por una máquina o por un humano.\n\nAntes de empezar descarguemos e importemos algunas dependecias."},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download es_core_news_lg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\nimport string\nimport operator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import es_core_news_lg\n\nfrom collections import defaultdict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/aichallenge2020/training.csv')\ntest_data  = pd.read_csv('../input/aichallenge2020/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hay que conocer los datos.."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_human_tweets = sum(train_data['is_organico'] == 1)\ntrain_bot_tweets   = sum(train_data['is_organico'] == 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (7, 5)\nfig = plt.figure()\nsns.barplot(x=['Human Tweets', 'Bot Tweets'], y = [train_human_tweets, train_bot_tweets])\nplt.title(\"Distribucón de los tweets en el set de entrnamiento\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ésta primera vizualización no enseña que los datos están imbalanceados. Luego nos encargaremos de éste problema.\n\nMiremos más a fondo."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef length(x):\n    return len(x)\n\ntrain_data['length'] = train_data['tweet'].apply(lambda x: length(x))\n\nbot_tweet_length = train_data[train_data['is_organico'] == 0]['length']\nhuman_tweet_length = train_data[train_data['is_organico'] == 1]['length']\n\nprint('Media de caracteres por tweet inorgánico: ', np.mean(list(bot_tweet_length)))\nprint('Media de caracteres por tweet orgánico: ', np.mean(list(human_tweet_length)))\n\nplt.figure(figsize=(10,10))\nplt.title('Bot and Human tweet length')\nsns.distplot(human_tweet_length)\nsns.distplot(bot_tweet_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los tweets inorganicos son en promedio más largos, investiguemos esto más a fondo. La maxima cantidad de caracteres en un tweet es de 280 carácteres. Tomemos este valor como umbral. "},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_length = 280\n\ntemp = train_data[train_data['is_organico'] == 0]\nbot_data = temp[temp['length'] > tweet_length]\n\ntemp_ = train_data[train_data['is_organico'] == 1]\nhuman_data = temp_[temp_['length'] > tweet_length]\n\nx = ['Organicos', 'No Organicos']\n\nplt.rcParams['figure.figsize'] = (7, 5)\nfig = plt.figure()\nsns.barplot(x=x, y = [human_data.shape[0], bot_data.shape[0]])\nplt.title(\"tweets mas largos de 280 carácteres\")\nprint(human_data.shape[0] + bot_data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mayoría de los tweets más largos son inorganicos. Aumentemos el umbral."},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_length = 300\n\ntemp = train_data[train_data['is_organico'] == 0]\nbot_data = temp[temp['length'] > tweet_length]\n\ntemp_ = train_data[train_data['is_organico'] == 1]\nhuman_data = temp_[temp_['length'] > tweet_length]\n\nx = ['Organicos', 'No Organicos']\n\nplt.rcParams['figure.figsize'] = (7, 5)\nfig = plt.figure()\nsns.barplot(x=x, y = [human_data.shape[0], bot_data.shape[0]])\nplt.title(\"tweets mas largos de 300 carácteres\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_length = 10\n\ntemp = train_data[train_data['is_organico'] == 0]\nbot_data = temp[temp['length'] < tweet_length]\n\ntemp_ = train_data[train_data['is_organico'] == 1]\nhuman_data = temp_[temp_['length'] < tweet_length]\n\nx = ['Organicos', 'No Organicos']\n\nplt.rcParams['figure.figsize'] = (7, 5)\nfig = plt.figure()\nsns.barplot(x=x, y = [human_data.shape[0], bot_data.shape[0]])\nplt.title(\"tweets mas largos de 300 carácteres\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nuestro primer análisis podemos llegar a la conclusión que los tweets mas largos tienden a ser inorganicos, mientras que los tweets más cortos tienden a ser orgánicos.\n\nAhora chequeemos si todos los tweets están en español."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from langdetect import detect\n\ndef detect_lang(text):\n  try:\n    return detect(text)\n  except:\n    return None\n\ntrain_data['lang'] = train_data['tweet'].apply(lambda x: detect_lang(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"esp_tweet = train_data[train_data['lang'] == 'es']\nno_esp_tweet = train_data[train_data['lang'] != 'es']\n\nesp_tweet_qty = esp_tweet.shape[0]\nno_esp_tweet_qty = no_esp_tweet.shape[0]\n\nplt.rcParams['figure.figsize'] = (7, 5)\nfig = plt.figure()\nsns.barplot(x=['Tweets en Español', \"Tweet en otro idioma\"], y = [esp_tweet_qty,no_esp_tweet_qty])\nplt.title(\"Languajes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tnemos algunos tweets que no están en español. Será importante tomar esto en cuenta."},{"metadata":{},"cell_type":"markdown","source":"De nuestro primer análisis exploratorio podemos concluir algunas cosas.\n\n* Los datos se encuentran imbalanceados.\n* Los tweets mas largos tienden a ser no organicos mientras que los más cortos tienden a ser orgánicos.\n\nDe ésta última conclusión podemos hacernos unas preguntas: \n\n**Los tweets tienden una cantidad máxima de carácteres de 280 carácteres, hay tweets con muchos más carácteres esto afecta la distribución del dataset. Ésto afectará el desempeño del modelo en entornos reales.**\n\n**Hay tweets en Inglés y en Español si el modelo estará expuesto a datos en multi-idioma no hay suficientes tweets en otro idioma, ésto afectaría el performance en producción**\n\n* La recolección de los datos no ha sido la mejor, éste es el stage más importante de la construcción de modelos de IA. La calidad de los datos no es buena.\n\n* Los tweets necesitarán de Data Cleanning, para eliminar caracteres especiales y otros datos que sean inncesarios."},{"metadata":{},"cell_type":"markdown","source":"> #  <h1>LIMPIEZA DE DATOS</h1>\n\nLa parte más compleja de trabajar con texto es que lo que para nosotros puede ser muy útil, para un modelo es solo ruido, por tal motivo es necesario remover todo ese ruido en los datos, lo cual mejorará el performance final del modelo.\n\n**¿Cuales son esos ruidos?** \n\nCaracteres especiales, palabras cortas y otras que solo entrenando al modelo se podrán identificar."},{"metadata":{},"cell_type":"markdown","source":"> #  <h1>NORMALIZACIÓN DE TEXTO</h1>\n\nNo solo los carácteres espeicales son ruido, sino las cojungaciones de verbos, palabras muy largas y conectivos, realizar este proceso tenemos... \n\nLematizacion, Stemming y Stop Words\n\n**Lematizacion**: La lematización es un proceso lingüístico que consiste en, dada una forma flexionada, hallar el lema correspondiente. El lema es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra https://es.wikipedia.org/wiki/Lematizaci%C3%B3n\n\n**Stemming**: Stemming es un método para reducir una palabra a su raíz o a un stem. Hay algunos algoritmos de stemming que ayudan en sistemas de recuperación de información. Stemming aumenta el recall que es una medida sobre el número de documentos que se pueden encontrar con una consulta. https://es.wikipedia.org/wiki/Stemming\n\n**Stop words (Palabras Vacias)**: Palabras vacías es el nombre que reciben las palabras sin significado como artículos, pronombres, preposiciones, etc. que son filtradas antes o después del procesamiento de datos en lenguaje natural. https://es.wikipedia.org/wiki/Palabra_vac%C3%ADa"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.head(20))\nprint(' ')\nprint('-----------------------------------------------------------------------------------------------')\nprint('-----------------------------------------------------------------------------------------------')\nprint('-----------------------------------------------------------------------------------------------')\n\ndef normalise_text(text):\n    text = text.lower().strip() # lowercase\n    text = re.sub(r'[0-9]', \" numero \", text)\n    text = re.sub(r\"#\\S+\",\"tema\", text) # replaces hashtags\n    text = re.sub(r'http\\S+', 'enlace', text)  # remove URL addresses\n    text = re.sub(r\"@\\S+\",\"nombre\", text)\n    text = re.sub(f\"[{re.escape(string.punctuation)}]\", ' ', text)\n    text = re.sub(r\"[^A-Za-z0-9ñ()!?\\'\\`\\\"]\", \" \", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text\n   \n\ndef remove_accents(text):\n    text = re.sub(u\"[á]\", \"a\", text)\n    text = re.sub(u\"[é]\", \"e\", text)\n    text = re.sub(u\"[í]\", \"i\", text)\n    text = re.sub(u\"[ó]\", \"o\", text)\n    text = re.sub(u\"[ú]\", \"u\", text)\n    return text\n    \n\n    \ndef remove_spaces(x):\n    try:\n        return ' '.join([word for word in x.split(' ') if word != ''])\n    except:\n        return None\n    \n\ndef all_prep(text):\n  text = remove_accents(text)\n  text = normalise_text(text)\n  text = remove_spaces(text)\n  return text\n\ntrain_data['tweet'] = train_data['tweet'].apply(lambda x: all_prep(x))\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora a manera de Baseline hagamos en una prueba, con una libreroa creada por Facebook llamada Fasttext, con ella podremos crear rapidamente, y además entrenar un modelo clasificador de texto."},{"metadata":{"trusted":true},"cell_type":"code","source":"import fasttext as ft\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rate = 0.3\n\nratio = int(len(train_data) * rate)\n\ntest = train_data[:ratio]\ntrain = train_data[ratio:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para crear el clasificador, la funcion de entrenamiento toma un archivo de texto con el datapoint, cada linea del archivo de texto tendrá el datapoint con su respectiva etiqueta."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dicty = {\n    \"data\": []\n}    \n\ntest_dicty = {\n    \"data\": []\n}    \n\nfor line in train_data.iterrows():\n    label = line[1]['is_organico']\n    text = line[1]['tweet']\n    for_write = \"__label__\" + str(label) + \" \" + str(text) + \"\\n\"\n    train_dicty['data'].append(for_write)\n    \nfor line in train_data.iterrows():\n    label = line[1]['is_organico']\n    text = line[1]['tweet']\n    for_write = \"__label__\" + str(label) + \" \" + str(text) + \"\\n\"\n    test_dicty['data'].append(for_write)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_file = open('train_data.txt','w')\ntraining_file.writelines(dicty['data'])\ntraining_file.close()\n\ntest_file = open('test_data.txt','w')\ntest_file.writelines(dicty['data'])\ntest_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier1 = ft.FastText.train_supervised('train_data.txt', lr=0.01, wordNgrams=1, epoch=15)\nclassifier2 = ft.FastText.train_supervised('train_data.txt', lr=0.001, wordNgrams=2, epoch=15)\nclassifier3 = ft.FastText.train_supervised('train_data.txt', lr=0.01, wordNgrams=3, epoch=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classifier3.test(\"test_data.txt\"))\nprint(classifier2.test(\"test_data.txt\"))\nprint(classifier1.test(\"test_data.txt\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Metricas Clasificador #1: ', classifier1)\nprint('Metricas Clasificador #2: ', classifier2)\nprint('Metricas Clasificador #3: ', classifier3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Asi de sencillo podemos crear un clasificador y hacer pruebas acerca de nuestro preprocesamiento de texto."},{"metadata":{},"cell_type":"markdown","source":"Todas las pruebas no estarán en éste notebook, por tal motivo estarán mejor explicadas en un docuemento adjunto.\n\nAhora procedamos con la construcción del modelo final será el \"roberta-base\".\n\nConsideraciones:\n\n* Los modelos preentrenados necesitan un learning rate alto, sin embargo, hemos puesto un clasificador propio el cual debemos entrenar con un learning rate mas alto, por tal motivo necesitaremos un learning rate más alto, por tal motivo entrenaremos unas pocas epocas con un alto learning rate y las capas del modelo roberta congeladas, luego entrenaremos con un learning rate más bajo.\n\n* ***language models are few shot learners*** en ese paper se señala la propiedad de trade off que tienen el batch size y el tamaño final de nuestras secuencias de textos, para evitar overfitting trabajaremos con secuencuias largas y un batch size más pequeño de lo usual.\n\n* No se utilizarán embedding pre-entrenados para asegurar que los embeddings capten las caractereristicas de nuestro problema"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nimport torch\nimport transformers\n\nimport torch.nn as nn\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nfrom transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/aichallenge2020/training.csv\")\ntest_data = pd.read_csv(\"../input/aichallenge2020/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta = RobertaModel.from_pretrained('roberta-base')\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Puesto que tenemos tweets con 4000 caracteres utilizaremos un max_length = 256 "},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_train = tokenizer.batch_encode_plus(\n    train_data['tweet'].to_list(),\n    max_length = 256,\n    pad_to_max_length=True,\n    truncation=True,\n)\n\ntokens_test = tokenizer.batch_encode_plus(\n    test_data['tweet'].to_list(),\n    max_length = 256,\n    pad_to_max_length= True,\n    truncation=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\n\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\n\ntest_ids = torch.tensor(test_data['Id'].to_list())\ntrain_y = torch.tensor(train_data['is_organico'].to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ya que utilizamos un longitud maxima utilizamos un batch size pequeño"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 8\n\n# wrap tensors\ntrain_dataset = TensorDataset(train_seq, train_mask, train_y)\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_dataset)\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size, num_workers=4, pin_memory=True)\n\n#Test data\ntest_dataset = TensorDataset(test_seq, test_mask, test_ids)\n#Sampler\ntest_sampler = SequentialSampler(test_dataset)\n#Loader\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ROBERT_Arch(nn.Module):\n  # Model with extra layers on top of RoBERTa\n    def __init__(self, dropout_rate=0.3):\n        super(ROBERT_Arch, self).__init__()\n        \n        self.roberta = RobertaModel.from_pretrained('roberta-base')\n        self.d1 = torch.nn.Dropout(dropout_rate)\n        self.l1 = torch.nn.Linear(768, 64)\n        self.bn1 = torch.nn.LayerNorm(64)\n        self.d2 = torch.nn.Dropout(dropout_rate)\n        self.l2 = torch.nn.Linear(64, 2)\n        \n    def forward(self, input_ids, attention_mask):\n        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        x = self.d1(x)\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = torch.nn.Tanh()(x)\n        x = self.d2(x)\n        x = self.l2(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ROBERT_Arch(0.4)\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW\n\n# define the optimizer\nparams = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]  \noptmizer_parameters = [\n                       {\"params\":[p for n, p in params if not any(nd in n for nd in no_decay)],\n                       \"weight_decay\": 0.001,},\n                       {\"params\":[p for n, p in params if any(nd in n for nd in no_decay)],\n                       \"weight_decay\": 0.0,},\n                       \n]   \n\noptimizer = AdamW(optmizer_parameters,\n                  lr = 1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recuerda que hablamos de clases imbalanceadas? pues solucionemos el problema"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', np.unique(train_data['is_organico'].values), train_data['is_organico'].values)\n\nprint(\"Class Weights:\", class_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\nweights= torch.tensor(class_weights, dtype=torch.float)\nepochs = 6\nsteps = len(train_dataloader)\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy = nn.CrossEntropyLoss(weight=weights)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=steps, num_training_steps=steps*12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(optimizer):\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n   \n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n    scheduler.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss / len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Entrenamos solo el clasificador del modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.roberta.parameters():\n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train(optimizer)\n    \n    train_losses.append(train_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora ya que entrenaremos el modelo completo y está pre entrenado pues, hagamoslo con un learning rate mucho más bajo."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]  \noptmizer_parameters = [\n                       {\"params\":[p for n, p in params if not any(nd in n for nd in no_decay)],\n                       \"weight_decay\": 0.001,},\n                       {\"params\":[p for n, p in params if any(nd in n for nd in no_decay)],\n                       \"weight_decay\": 0.0,},\n                       \n]   \n\noptimizer = AdamW(optmizer_parameters,\n                  lr = 3e-6)\nepochs = 14","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.roberta.parameters():\n  param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train(optimizer)\n    \n    train_losses.append(train_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dicty = {\n    \"Id\": [],\n    \"Predicted\":[]\n    }\nwith torch.no_grad():\n  for batch in test_dataloader:\n    batch = [r.to(device) for r in batch]\n\n    seq, mask, id = batch\n    output = model(seq, mask)\n    preds = np.argmax(output.cpu().detach().numpy(), axis = 1)\n    for pred, ids in zip(preds, id):\n      data_dicty['Predicted'].append(pred)\n      data_dicty['Id'].append(int(ids.item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('./pr.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame(data_dicty)\ndata.to_csv(\"pre.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'roberta_saved_weights_finañ.pt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Función para inferencia en dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(csv_file, x, y, batch_size, model):\n    \"\"\"\n    x: [str] Nombre de la columna del datapoint en el dataframe\n    y; [str] Nombre de la columna label en el dataframe\n    csv_file: [str] ruta del archivo csv del dataset\n    \"\"\"\n    data = pd.read_csv(csv_file)\n    \n    tokens = tokens_train = tokenizer.batch_encode_plus(\n    data[x].to_list(),\n    max_length = 256,\n    pad_to_max_length=True,\n    truncation=True\n    )\n    \n    data_seq = torch.tensor(tokens['input_ids'])\n    data_mask = torch.tensor(tokens['attention_mask'])\n    data_y = torch.tensor(data[y].to_list())\n    \n    test_dataset = TensorDataset(data_seq, data_mask, data_y)\n    #Sampler\n    test_sampler = SequentialSampler(test_dataset)\n    #Loader\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n    \n    data_dicty = {\n    \"label\": [],\n    \"Predicted\":[]\n    }\n    with torch.no_grad():\n        \n      for batch in test_dataloader:\n        batch = [r.to(device) for r in batch]\n        seq, mask, labels = batch\n        output = model(seq, mask)\n        preds = np.argmax(output.cpu().detach().numpy(), axis = 1)\n        \n        for pred, label in zip(preds, labels):\n          data_dicty['Predicted'].append(pred)\n          data_dicty['label'].append(int(label.item()))\n          \n      return data_dicty","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}